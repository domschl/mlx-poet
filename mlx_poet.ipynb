{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fEXNOWhCEAPk"
   },
   "source": [
    "# MLX-Transformer-Poet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DabS0VZ-1Zp0"
   },
   "source": [
    "Please review [ml-indie-tools](https://github.com/domschl/ml-indie-tools), a collection machine learning tools that provides support for more environment indepent code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "gF-7qFzMdnN1",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ml-indie-tools in ./lib/python3.12/site-packages (0.12.20)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U ml-indie-tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "EgLLjG4yQtft"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import copy\n",
    "import json\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "from zoneinfo import ZoneInfo\n",
    "\n",
    "from functools import partial\n",
    "import mlx\n",
    "import mlx.core as mx\n",
    "import mlx.core as mx\n",
    "import mlx.nn as nn\n",
    "import mlx.optimizers as optim\n",
    "import numpy as np\n",
    "from mlx.utils import tree_flatten\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "# import ipywidgets as widgets\n",
    "# from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "U5T4m6earb1e"
   },
   "outputs": [],
   "source": [
    "from ml_indie_tools.env_tools import MLEnv\n",
    "from ml_indie_tools.Gutenberg_Dataset import Gutenberg_Dataset\n",
    "from ml_indie_tools.Text_Dataset import Text_Dataset\n",
    "\n",
    "from ml_indie_tools.Calibre_Dataset import Calibre_Dataset\n",
    "from ml_indie_tools.Folder_Dataset import Folder_Dataset\n",
    "from ml_indie_tools.train_utils import TrainUtils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indralib is available, trying to connect to Indrajala server for training progress reports...\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import indralib\n",
    "    indra_avail = True\n",
    "except Exception as e:\n",
    "    indra_avail = False\n",
    "if indra_avail is True:\n",
    "        print(\"Indralib is available, trying to connect to Indrajala server for training progress reports...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "jVcwvURB5EZN"
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "log = logging.Logger(\"Main\")\n",
    "log.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sfZg31sMEAP1"
   },
   "source": [
    "## 0. Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "llPw84PkEAP2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'OS: Darwin, Python: 3.12.3, Jupyter Notebook MLX: 0.12.2, GPU: MLX GPU (system memory)'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cached_batch_data = None   # Do regenerate time-consuming training data, if aleady cached.\n",
    "\n",
    "ml_env = MLEnv(platform='mlx', accelerator='fastest')\n",
    "ml_env.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Qg3ZPBmC8kO"
   },
   "source": [
    "## 1. Project configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "t-TP3Pnsrb1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root path (all projects) : . (This will be '.' (current dir) for local projects, and a google drive path for Colab)\n",
      "Project path             : . (Changes to the file system happen only below this project path\n",
      "Model path (snapshots)   : ./model/ng_COMP_neo_philosophers_v2_pt (Model weights and snapshots are stored here)\n",
      "Data path (training data): ./data (Training data will be downloaded here)\n",
      "Log dir (tensorboard)    : ./logs (it doesn't work to put logs on gdrive due to caching, hence local dir)\n"
     ]
    }
   ],
   "source": [
    "project_name = 'neo_philosophers'\n",
    "# project_name='women_writers'\n",
    "model = None\n",
    "optimizer = None\n",
    "model_name=f'ng_COMP_{project_name}_v2_pt'\n",
    "\n",
    "use_preprocessed_data = True                      # Use already tokenized data\n",
    "use_existing_model_from_checkpoint = False        # Try to load checkpoint of training\n",
    "skip_additional_texts = True                      # Don't look for other data sources in `additional_texts.json`\n",
    "# NOTICE: This will request access to Google Drive, if running on Google Colab. Google Drive is used to store snapshots\n",
    "# training data. See project ml-indie-tools: https://github.com/domschl/ml-indie-tools\n",
    "#\n",
    "# Note: you need to allow popups in your browser for COLAB, otherwise you won't see the google-drive login box, and drive access will fail!\n",
    "\n",
    "root_path, project_path, model_path, data_path, log_path = ml_env.init_paths(project_name=project_name, model_name=model_name)\n",
    "\n",
    "print(f\"Root path (all projects) : {root_path} (This will be '.' (current dir) for local projects, and a google drive path for Colab)\")\n",
    "print(f\"Project path             : {project_path} (Changes to the file system happen only below this project path\")\n",
    "print(f\"Model path (snapshots)   : {model_path} (Model weights and snapshots are stored here)\")\n",
    "print(f\"Data path (training data): {data_path} (Training data will be downloaded here)\")\n",
    "print(f\"Log dir (tensorboard)    : {log_path} (it doesn't work to put logs on gdrive due to caching, hence local dir)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AIkcYcEuQtfx"
   },
   "source": [
    "##  2.1 Text data from Project Gutenberg\n",
    "\n",
    "`Text_Dataset` and `Gutenberg_Dataset` classes: libraries for training,\n",
    "encoding, batch generation, and formatted source display. It read some\n",
    "books from Project Gutenberg and supports creation of training batches.\n",
    "The output functions support highlighting to allow to compare generated\n",
    "texts with the actual sources to help to identify identical (memorized)\n",
    "parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "HjkelBcNO5WV"
   },
   "outputs": [],
   "source": [
    "use_dark_mode=False # Set to false for white background. HTML-text-compare uses background-colorization to identify different sources. Those background colors are dependent on the theme type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "BF8eyWnCrb1h"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Datasets:Loading tokenizer from ./data/neo_philosophers_tokens.json\n",
      "INFO:Datasets:Loading tokenizer done.\n"
     ]
    }
   ],
   "source": [
    "token_file = os.path.join(data_path,f\"{project_name}_tokens.json\")\n",
    "if use_preprocessed_data is True:\n",
    "    if os.path.exists(token_file):\n",
    "        td = Text_Dataset()\n",
    "        td.load_tokenizer(token_file)\n",
    "    else:\n",
    "        print(f\"Can't use preprocessed data, since {token_file} is missing, preprocessing data...\")\n",
    "        use_preprocessed_data = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "C66X7ynnrb1h"
   },
   "outputs": [],
   "source": [
    "if use_preprocessed_data is False:\n",
    "    cache_dir = os.path.join(data_path, 'gutenberg_cache')\n",
    "    gd = Gutenberg_Dataset(cache_dir=cache_dir)\n",
    "\n",
    "    if project_name == 'women_writers':  # sample searches\n",
    "        search_spec= {\n",
    "            \"author\": [\"Emily Brontë\", \"Jane Austen\", \"Virginia Woolf\"],\n",
    "            \"language\": [\"english\"]\n",
    "        }\n",
    "        book_list=gd.search(search_spec)\n",
    "    elif project_name == 'neo_philosophers':\n",
    "        search_spec = {\n",
    "            \"author\": [\"Immanuel Kant\", \"Friedrich Nietzsche\", \"Wilhelm Hegel\", \"Arthur Schopenhauer\"],\n",
    "            \"language\": [\"english\", \"german\"]\n",
    "        }\n",
    "        book_list=gd.search(search_spec)\n",
    "        search_spec = {\n",
    "            \"author\": [\"Plato\", \"Platon\"],\n",
    "            \"title\": [\"Timaeus\", \"Critias\", \"Symposium\"],\n",
    "            \"language\": [\"english\", \"german\"]\n",
    "        }\n",
    "        book_list+=gd.search(search_spec)\n",
    "        search_spec = {\n",
    "            \"title\": [\"Buddh\", \"Sutra\"],\n",
    "            \"language\": [\"english\", \"german\"]\n",
    "        }\n",
    "        book_list+=gd.search(search_spec)\n",
    "    else:\n",
    "        search_spec = {}\n",
    "        book_list = []\n",
    "\n",
    "    book_cnt = len(book_list)\n",
    "    print(f\"{book_cnt} matching books found with search {search_spec}.\")\n",
    "\n",
    "    if book_cnt > 0:\n",
    "        if book_cnt<80:\n",
    "            # Note: please verify that book_cnt is 'reasonable'. If you plan to use a large number of texts,\n",
    "            # consider [mirroring Gutenberg](https://github.com/domschl/ml-indie-tools#working-with-a-local-mirror-of-project-gutenberg)\n",
    "            book_list = gd.insert_book_texts(book_list, download_count_limit=book_cnt)\n",
    "        else:\n",
    "            logging.error(\"Please verify your book_list, a large number of books is scheduled for download. ABORTED.\")\n",
    "\n",
    "        for i in range(len(book_list)):\n",
    "            if 'author' not in book_list[i]:\n",
    "                book_list[i]['author']='unknown'\n",
    "            print(f\"{i}: {book_list[i]['title']} - {book_list[i]['author']}, {book_list[i]['ebook_id']}\")\n",
    "\n",
    "        if project_name == 'women_writers':\n",
    "            select = (\"Bennett\", \"1342\", \"5670\", \"1245\", \"161\", \"141\", \"121\", \"105\", \"Susan\", \"Wuthering\", \"Emma\", \"Voyage\")  # List unique single-words from title or ebook_id to select a given book\n",
    "            sub_book_list = [book_list[i] for i in range(len(book_list)) if not set([book_list[i]['ebook_id']]+book_list[i]['title'].split(' ')).isdisjoint(set(select))]\n",
    "        else:\n",
    "            sub_book_list = book_list\n",
    "\n",
    "        print(\"Using:\")\n",
    "        for i in range(len(sub_book_list)):\n",
    "            if 'author' not in sub_book_list[i]:\n",
    "                sub_book_list[i]['author']='unknown'\n",
    "            print(f\"{i+1}: {sub_book_list[i]['title']} - {sub_book_list[i]['author']}\")\n",
    "\n",
    "        td = Text_Dataset(sub_book_list)\n",
    "    else:\n",
    "        td = Text_Dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dxNIc7gL9UNg"
   },
   "source": [
    "## 2.2 Additional training material from folders or Calibre library\n",
    "\n",
    "This looks for a file `additional_texts.json` in the `project_path` as shown above.\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"local_texts\": [\"/some/directory/that/contains/texts\"],\n",
    "  \"calibre\": \"/home/myuser/Calibre Library\"\n",
    "}\n",
    "```\n",
    "\n",
    "If the folder(s) defined in `local_texts` contain text files with default endings `.txt`, `.md`, `.org`, or `.py` (can be configured), they are added to the training data. Folders are searched recursively.\n",
    "\n",
    "If the path defined in `calibre` contains a Calibre database, all text files (`.txt` only) within that library are added to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "1NYdjlW65EZP"
   },
   "outputs": [],
   "source": [
    "if use_preprocessed_data is False and skip_additional_texts is False:\n",
    "    additional = os.path.join(project_path, \"additional_texts.json\")\n",
    "    print(f\"Looking for description of additional sources in {additional}\")\n",
    "    if os.path.exists(additional) is True:\n",
    "        with open(additional, 'r') as f:\n",
    "            add_desc = json.load(f)\n",
    "            if 'local_texts' in add_desc:\n",
    "                fd = Folder_Dataset()\n",
    "                for text_path in add_desc['local_texts']:\n",
    "                    print(f\"Loading texts from {text_path}\")\n",
    "                    fd.load_index(text_path, use_aliases=False, max_file_size=100000)\n",
    "                td.load_texts(fd.records[:10000])\n",
    "            if 'calibre' in add_desc:\n",
    "                cal_path = add_desc['calibre']\n",
    "                if os.path.exists(cal_path):\n",
    "                    print(f\"Loading text from calibre at {cal_path}\")\n",
    "                    cd = Calibre_Dataset(cal_path)\n",
    "                    cd.load_index(max_file_size=100000000)\n",
    "                    td.load_texts(cd.records[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GSm4f9NSC8kQ"
   },
   "source": [
    "## 2.3 Tokenize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "bsyBjqFyC8kQ"
   },
   "outputs": [],
   "source": [
    "if use_preprocessed_data is False:\n",
    "    MAX_TOKENS = 10000  # This becomes vocab_size\n",
    "    MAX_NGRAM_LEN = 4   # Max length of a token\n",
    "    CHUNK_SIZE = 500000 # Split larger texts in chunks, if not None\n",
    "\n",
    "    print(\"\")\n",
    "    print(f\"Starting tokenizer with token length from 1..{MAX_NGRAM_LEN} with a max of {MAX_TOKENS} unique tokens,\")\n",
    "    print(\"this can take considerable time...\")\n",
    "\n",
    "    # td.init_tokenizer(tokenizer='ngram', max_ngrams=MAX_NGRAM_LEN, max_tokens=MAX_TOKENS) # or 'bytegram'\n",
    "    td.init_tokenizer(tokenizer='bytegram', max_ngrams=MAX_NGRAM_LEN, max_tokens=MAX_TOKENS, chunk_size=CHUNK_SIZE)\n",
    "    td.save_tokenizer(token_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MG03WA_yC8kR"
   },
   "source": [
    "## 3. Model metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "UPMwIn2gC8kR"
   },
   "outputs": [],
   "source": [
    "params = None\n",
    "updatable_keys=['learning_rate', 'batch_size', 'current_epoch', 'current_loss',\n",
    "                 'sample_every_n_iterations', 'sample_size', 'save_every_n_iterations', 'max_iterations']\n",
    "attn_layers = 12\n",
    "embs = 256\n",
    "seq_len = 128\n",
    "\n",
    "params = { # Multi-head self-attention\n",
    "        'meta_name_template': '{mhsa_layers}x{heads}x{units}x{vocab_size}',\n",
    "\n",
    "        'layers': attn_layers,\n",
    "        'heads': 8,\n",
    "        # 'causal': True,  # Use causal self-attention\n",
    "        # 'dropout': 0.1,\n",
    "        'vocab_size': td.get_unique_token_count(),\n",
    "        'sequence_len': seq_len,\n",
    "        'embedding_size': embs,\n",
    "        'test_iterations': 20,  # number of epocs for loss estimation\n",
    "        'checkpoint': True,  # MLX gradient checkpointing\n",
    "        'context_sub_layers': attn_layers // 2,\n",
    "        'use_recur': False,\n",
    "\n",
    "        'joint_state_training': 0,\n",
    "        'batch_size': 64,\n",
    "        'learning_rate': 4e-4,\n",
    "        'weight_decay': 1e-6,\n",
    "        'lr_warmup': 100,  # num iterations for lr warmup\n",
    "\n",
    "        'sample_every_n_iterations': 1024,\n",
    "        # 'sample_size': 150,\n",
    "        # 'save_every_n_iterations': 1024,\n",
    "\n",
    "        'max_iterations': 10000000  # maximum number of training iterations\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5U1R4yDlC8kR"
   },
   "source": [
    "## 4. Batch handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "f7_tc2Lirb1i"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15392802 records\n"
     ]
    }
   ],
   "source": [
    "td.init_getitem(sample_type='encoded', sample_length=params['sequence_len']+1, content_stepping=1)\n",
    "num_records = len(td)\n",
    "print(f\"{num_records} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "zZbbsNm0cOeW"
   },
   "outputs": [],
   "source": [
    "def get_sample_sub_batch(sample_batch, batch_size, sub_index=0):\n",
    "    for i in range(batch_size):\n",
    "        Xi = sample_batch[sub_index:-1-params['joint_state_training']+sub_index]\n",
    "        if params['joint_state_training']+sub_index == 0:\n",
    "            yi = sample_batch[sub_index+1:]\n",
    "        else:\n",
    "            yi = sample_batch[sub_index+1:-params['joint_state_training']+sub_index]\n",
    "        if i==0:\n",
    "            # smpX=np.array(Xi, dtype=np.float32)\n",
    "            smpX=np.array(Xi, dtype=np.uint32)\n",
    "            smpy=np.array(yi, dtype=np.uint32)\n",
    "        else:\n",
    "            # smpX = np.vstack((smpX, np.array(Xi, dtype=np.float32)))\n",
    "            smpX = np.vstack((smpX, np.array(Xi, dtype=np.uint32)))\n",
    "            smpy = np.vstack((smpy, np.array(yi, dtype=np.uint32)))\n",
    "    return np.array(smpX, dtype=np.uint32), np.array(smpy, dtype=np.uint32)\n",
    "\n",
    "def get_sample_batch(td, batch_size):\n",
    "    sample_batch = td.get_random_item()\n",
    "    return get_sample_sub_batch(sample_batch, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "jY3hUuhQYzdT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_batches = 240512\n"
     ]
    }
   ],
   "source": [
    "num_batches = num_records // params['batch_size']\n",
    "print(f\"num_batches = {num_batches}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "ADm9ycuA2ik7"
   },
   "outputs": [],
   "source": [
    "# Adapted from: https://raw.githubusercontent.com/ml-explore/mlx-examples/main/transformer_lm/main.py\n",
    "class TransformerLM(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        num_layers: int,\n",
    "        dims: int,\n",
    "        num_heads: int,\n",
    "        checkpoint: bool,\n",
    "        context_sub_layers: int,\n",
    "        use_recur: bool,\n",
    "        sequence_length: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.use_recur = use_recur\n",
    "        self.context_sub_layers = context_sub_layers\n",
    "        self.sequence_length = sequence_length\n",
    "        self.embed = nn.Embedding(vocab_size, dims)\n",
    "        self.pe = nn.SinusoidalPositionalEncoding(dims)\n",
    "\n",
    "        if use_recur is False:\n",
    "            self.transformer = nn.TransformerEncoder(\n",
    "                num_layers, dims, num_heads, norm_first=True, checkpoint=checkpoint\n",
    "            )\n",
    "        else:\n",
    "            if context_sub_layers == -1 or context_sub_layers == 0:\n",
    "                context_sub_layers = num_layers // 2;\n",
    "            self.transformer = nn.TransformerEncoder(\n",
    "                context_sub_layers, dims, num_heads, norm_first=True, checkpoint=checkpoint\n",
    "            )\n",
    "            self.transformer2 = nn.TransformerEncoder(\n",
    "                num_layers - context_sub_layers, dims, num_heads, norm_first=True, checkpoint=checkpoint\n",
    "            )\n",
    "            self.rnn1 = nn.RNN(dims, dims) \n",
    "            self.rnn2 = nn.RNN(dims, dims)\n",
    "            self.rnn3 = nn.RNN(dims, dims)\n",
    "            self.rnn4 = nn.RNN(dims, dims)\n",
    "            self.rk = nn.Linear(dims, dims)\n",
    "            self.rv = nn.Linear(dims, dims)\n",
    "            self.rq = nn.Linear(dims, dims)\n",
    "            # self.lstm3 = nn.LSTM(dims, dims)\n",
    "            # self.lstm4 = nn.LSTM(dims, dims)\n",
    "            self.lnorm = nn.LayerNorm(dims)\n",
    "            self.proj1 = nn.Linear(dims, dims)\n",
    "\n",
    "        self.out_proj = nn.Linear(dims, vocab_size)\n",
    "\n",
    "\n",
    "    def __call__(self, x):\n",
    "        L = x.shape[1]\n",
    "        mask = nn.MultiHeadAttention.create_additive_causal_mask(L)\n",
    "        x = self.embed(x)\n",
    "        x = x + self.pe(mx.arange(L))\n",
    "        x = self.transformer(x, mask)\n",
    "        \n",
    "        if self.use_recur is True:\n",
    "            skip=x\n",
    "            # Add some kind of persistent state using recurrence layers -----\n",
    "            i_shape = x.shape        \n",
    "            x = self.rnn1(x) + x\n",
    "            x = self.rnn2(x) + x\n",
    "            x = self.rnn3(x) + x\n",
    "            x = self.rnn4(x) + x\n",
    "            # Use attention with last transformer output as query on the 'memory'\n",
    "            xk = self.rk(x)\n",
    "            xv = self.rv(x)\n",
    "            xq = self.rq(skip)\n",
    "            att = mx.matmul(nn.softmax(mx.matmul(xq, xk.transpose(0,2,1))/math.sqrt(L), axis=2), xv)\n",
    "            x = self.lnorm(att)\n",
    "            x = nn.softmax(self.proj1(x), axis=2)\n",
    "            # Merge 'memory' with residual:    \n",
    "            x = x + skip\n",
    "            x = self.transformer2(x, mask)\n",
    "            \n",
    "        return self.out_proj(x)\n",
    "\n",
    "    def embedding(self, x):\n",
    "        x = self.embed(x)\n",
    "\n",
    "    def context(self, x):\n",
    "        L = x.shape[1]\n",
    "        mask = nn.MultiHeadAttention.create_additive_causal_mask(L)\n",
    "        x = self.embedding(x)\n",
    "        x = x + self.pe(mx.arange(L))\n",
    "        x = self.transformer(x, mask)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_batches(batch_size, context_size, dataset):\n",
    "    while True:\n",
    "        x,y = get_sample_batch(dataset, batch_size)\n",
    "        yield x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, idx, max_new_tokens=128, temperature=1.0, multinomial_trials=20): # , top_k=None):\n",
    "        \"\"\"Generate new tokens given a context\n",
    "        :param idx: the context (B,T) tensor of indices\n",
    "        :param max_new_tokens: the maximum number of tokens to generate\n",
    "        :param temperature: the temperature to use for sampling\n",
    "        \"\"\"\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for i in range(max_new_tokens):\n",
    "            # crop idx to the last sequence_len tokens\n",
    "            idx = idx[:, -params['sequence_len'] :]\n",
    "            idx_mx = mx.array(idx)\n",
    "            logits = model(idx_mx)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :]  # becomes (B, C)\n",
    "            # apply temperature\n",
    "            if temperature != 1.0 and temperature > 0.0:\n",
    "                logits = logits / temperature\n",
    "            # apply softmax to get probabilities\n",
    "            probs = mx.softmax(logits, axis=1)  # XXX Check, if MLX understands -1\n",
    "\n",
    "            # Workaround for strange Numpy casting effects that cause sum(probs)>1:\n",
    "            probs = np.array(probs).astype('float64')\n",
    "            probs = probs / np.sum(probs)\n",
    "            idx_next = np.random.multinomial(multinomial_trials, probs[0]).argmax()\n",
    "            idx = np.concatenate((idx, [[idx_next]]), axis=1) \n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_prompt(model, td, prompt, pad=False):\n",
    "    if pad is True:\n",
    "        while len(prompt)<params['sequence_len']*5:  # XXX bad hack: after encoding, idx must be >= sequence_len, or generate fails\n",
    "            if len(prompt)==params['sequence_len']*5-1:\n",
    "                prompt = '\\n' + prompt\n",
    "            else:\n",
    "                prompt = ' ' + prompt\n",
    "    idx = np.array([td.encode(prompt)], dtype=np.uint32)\n",
    "    return idx\n",
    "    \n",
    "\n",
    "def generate_sample(model, td, prompt=' ', toks=128, state=None, temperature=1.0, pad=True, multinomial_trials=32):\n",
    "    idx = encode_prompt(model, td, prompt, pad)\n",
    "    answer = generate(model, idx, max_new_tokens=toks, temperature=temperature, multinomial_trials=multinomial_trials)\n",
    "    txt = td.decode(answer[0].tolist())\n",
    "    td.source_highlight(txt, min_quote_size=10, dark_mode=False, display_ref_anchor=False)\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:ml_indie_tools.train_utils:Starting indra thread\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profiles: ['default']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:ml_indie_tools.train_utils:Connected to Indrajala server with profile: default\n",
      "INFO:ml_indie_tools.train_utils:Logged in to Indrajala as user: indra, session id: a5231d8a-5f14-476d-adc9-7e87840b595d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login result: \"OK\", string, a5231d8a-5f14-476d-adc9-7e87840b595d\n"
     ]
    }
   ],
   "source": [
    "if indra_avail is True:\n",
    "    with open('indra_creds.json', 'r') as f:\n",
    "        creds = json.load(f)\n",
    "        tu = TrainUtils(indra_server_profile='default', username=creds['username'], password=creds['password'])\n",
    "else:\n",
    "    tu = TrainUtils()\n",
    "\n",
    "tu.train_session_start(model_name=model_name, model_description=\"MLX-poet tests\", model_version=1, model_params=params, indra_subdomain=\"mlx_poet/first_tests/1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_state(epoch, sub_batch, batches, loss):\n",
    "    # status = f\"{epoch} {sub_batch}/{batches}: {loss}\" \n",
    "    status, record = tu.train_state(epoch, sub_batch, batches, loss)\n",
    "    return status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "abort_training = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "ADm9ycuA2ik7"
   },
   "outputs": [],
   "source": [
    "def train(reset_model = False):\n",
    "    global model\n",
    "    global optimizer\n",
    "    batch_size = params['batch_size']\n",
    "    context_size = params['sequence_len']\n",
    "    steps_per_eval = params['test_iterations']\n",
    "    steps_per_report = params['sample_every_n_iterations']\n",
    "    loss_history_x = np.array([])\n",
    "    loss_history_y = np.array([])\n",
    "    max_loss_hist=1000\n",
    "    loss_history_interval = 25\n",
    "\n",
    "    # Initialize model:\n",
    "    if model is None or reset_model is True:\n",
    "        model = TransformerLM(\n",
    "            params['vocab_size'], params['layers'], params['embedding_size'], params['heads'], \n",
    "            params['checkpoint'], context_sub_layers=params['context_sub_layers'], use_recur=params['use_recur'],\n",
    "            sequence_length=params['sequence_len']\n",
    "        )\n",
    "        print(\"Created new model\")\n",
    "    else:\n",
    "        print(\"Reusing existing model, continuing training\")\n",
    "        \n",
    "    mx.eval(model.parameters())\n",
    "    nparams = sum(\n",
    "        x.size for k, x in tree_flatten(model.parameters()) if \"embedding\" not in k\n",
    "    )\n",
    "    print(f\"Training a transformer with {nparams / 1024**2:.3f} M parameters\")\n",
    "\n",
    "    def loss_fn(model, x, y, reduce=True):\n",
    "        # print(f\"loss-input-shape: {x.shape}, {x.dtype}\")\n",
    "        logits = model(x)\n",
    "        losses = nn.losses.cross_entropy(logits, y)\n",
    "        return mx.mean(losses) if reduce else mx.mean(losses, axis=(-1, -2))\n",
    "\n",
    "    if optimizer is None or reset_model is True:\n",
    "        optimizer = optim.AdamW(\n",
    "            learning_rate=params['learning_rate'], weight_decay=params['weight_decay']\n",
    "        )\n",
    "\n",
    "    # def eval_fn(dataset):\n",
    "    #     inputs, targets = map(mx.array, to_samples(context_size, dataset))\n",
    "    #     loss = 0\n",
    "    #     for s in range(0, targets.shape[0], batch_size):\n",
    "    #         bx, by = inputs[s : s + batch_size], targets[s : s + batch_size]\n",
    "    #         bx, by = map(mx.array, (bx, by))\n",
    "    #         losses = loss_fn(model, bx, by, reduce=False)\n",
    "    #         loss += mx.sum(losses).item()\n",
    "    #     return loss / len(targets)\n",
    "\n",
    "    state = [model.state, optimizer.state]\n",
    "\n",
    "    # compile disabled until https://github.com/ml-explore/mlx/issues/1063 is fixed. (or recurrent layer is removed)\n",
    "    @partial(mx.compile, inputs=state, outputs=state)\n",
    "    def step(inputs, targets):\n",
    "        loss_and_grad_fn = nn.value_and_grad(model, loss_fn)\n",
    "        loss, grads = loss_and_grad_fn(model, inputs, targets)\n",
    "        optimizer.update(model, grads)\n",
    "        return loss\n",
    "\n",
    "    train_iterator = iterate_batches(batch_size, context_size, td)\n",
    "    losses = np.array([])\n",
    "    tic = time.perf_counter()\n",
    "    last_report = time.time()\n",
    "\n",
    "    # irq_checkbox = widgets.Checkbox(description=\"Interrupt training\")\n",
    "    # display(irq_checkbox)\n",
    "    \n",
    "    for it, (inputs, targets) in zip(range(params['max_iterations']), train_iterator):\n",
    "        # abort_training = irq_checkbox.value\n",
    "        if abort_training is True:\n",
    "            break\n",
    "        inputs, targets = map(mx.array, (inputs, targets))\n",
    "        optimizer.learning_rate = min(1, it / params['lr_warmup']) * params['learning_rate']\n",
    "        loss = step(inputs, targets)\n",
    "        mx.eval(state)\n",
    "        losses = np.append(losses, ([loss.item()]))\n",
    "        # if (it+1) % loss_history_interval == 0:\n",
    "        #     loss_history_x = np.append(loss_history_x, ([it]))[-max_loss_hist:]\n",
    "        #     loss_history_y = np.append(loss_history_y, ([np.mean(losses[-loss_history_interval:])]))[-max_loss_hist:]\n",
    "        # pbar = TrainUtils.progress_bar_string(it%steps_per_report, steps_per_report, 20)\n",
    "        # print(f\"{it:6d} ⦊{pbar}⦉ loss: {np.mean(losses):.4f}    \", end=\"\\r\")\n",
    "        if time.time()-last_report > 1:\n",
    "            status_string = train_state(0, it%steps_per_report, steps_per_report, loss.item())\n",
    "            last_report = time.time()\n",
    "            print(status_string, end=\"\\r\")\n",
    "        if (it + 1) % steps_per_report == 0:\n",
    "            train_loss = np.mean(losses)\n",
    "            toc = time.perf_counter()\n",
    "            print(\n",
    "                f\"Iter {it + 1}: Train loss {train_loss:.3f}, \"\n",
    "                f\"It/sec {steps_per_report / (toc - tic):.3f}\"\n",
    "            )\n",
    "            # print(\"Gen: \")\n",
    "            # fig = plt.figure(figsize=(5, 2))\n",
    "            # plt.plot(loss_history_x, loss_history_y)\n",
    "            # plt.show()\n",
    "            prompt=\"At the beginning of time, the nature of reality was \"\n",
    "            print()\n",
    "            print()\n",
    "            print()\n",
    "            print(\"Generate:\")\n",
    "            txt = generate_sample(model, td, prompt=prompt, temperature=0.75)\n",
    "            print(f\"{prompt}{txt}\")\n",
    "            \n",
    "            # print(f\"Res: {txt}\")\n",
    "            losses = []\n",
    "            tic = time.perf_counter()\n",
    "        # if (it + 1) % steps_per_eval == 0:\n",
    "        #     val_loss = eval_fn(valid)\n",
    "        #     toc = time.perf_counter()\n",
    "        #     print(\n",
    "        #         f\"Iter {it + 1}: \"\n",
    "        #         f\"Val loss {val_loss:.3f}, \"\n",
    "        #         f\"Val ppl {math.exp(val_loss):.3f}, \"\n",
    "        #         f\"Val took {(toc - tic):.3f}s, \"\n",
    "        #     )\n",
    "            tic = time.perf_counter()\n",
    "\n",
    "    # if args.eval_test:\n",
    "    #     test_loss = eval_fn(test)\n",
    "    #     test_ppl = math.exp(test_loss)\n",
    "    #     print(f\"Test loss {test_loss:.3f}, Test ppl {test_ppl:.3f}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created new model\n",
      "Training a transformer with 13.919 M parameters\n",
      "    99 ⦊██                  ⦉ loss: 7.6999    \r"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    train(reset_model = False)\n",
    "except KeyboardInterrupt as e:\n",
    "    print(f\"Training interrupted: {e}\")\n",
    "tu.train_session_end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# txt = generate_sample(model, td, prompt=\"At the beginning of time \", pad=True, temperature=0.7)\n",
    "# print(f\"Res: {txt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": "ec3a4d2d-8063-4bfd-a4a2-ee070d3272f7",
   "lastKernelId": "1acc2b74-f51e-477b-910a-a5519dad53b9"
  },
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "VmWbteSFQtfq",
    "yWE_ZZMKEARV"
   ],
   "gpuClass": "premium",
   "gpuType": "V100",
   "include_colab_link": true,
   "machine_shape": "hm",
   "name": "torch_transformer_poet.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
